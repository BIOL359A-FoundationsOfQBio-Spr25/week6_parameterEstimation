{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Parameter Estimation and Regularization\n",
    "### Spring 2025, Week 6\n",
    "Objectives:\n",
    "- gain intuition for parameter estimation strategy\n",
    "- gain intuition for cost function landscapes\n",
    "- contextualize MLR parameters (coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ipywidgets import interact, widgets\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data\n",
    "Today we will start by working with in-silico data. The code below will generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, degree=3, noise_level=0.5, x_range=(-3, 3)):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with polynomial relationship and controlled noise.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    degree : int\n",
    "        True polynomial degree of the data\n",
    "    noise_level : float\n",
    "        Standard deviation of the Gaussian noise\n",
    "    x_range : tuple\n",
    "        Range of x values (min, max)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples,)\n",
    "        Feature values\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values with noise\n",
    "    true_coef : ndarray\n",
    "        True coefficients used to generate data\n",
    "    \"\"\"\n",
    "    # Generate random x values within the specified range\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(x_range[0], x_range[1], n_samples)\n",
    "    \n",
    "    # Generate random coefficients for polynomial\n",
    "    true_coef = np.random.randn(degree + 1)\n",
    "    true_coef = true_coef / np.max(np.abs(true_coef)) * 3  # Scale coefficients\n",
    "    \n",
    "    # Generate y values based on polynomial relationship\n",
    "    y_true = np.zeros(n_samples)\n",
    "    for i in range(degree + 1):\n",
    "        y_true += true_coef[i] * X**i\n",
    "    \n",
    "    # Add noise\n",
    "    y = y_true + noise_level * np.random.randn(n_samples)\n",
    "    X = X.reshape(-1, 1)\n",
    "    return X, y, true_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type='None', alpha=1.0, l1_ratio=0.5):\n",
    "    \"\"\"Create model based on regularization choice.\"\"\"\n",
    "    if model_type == 'Lasso':\n",
    "        return Lasso(alpha=alpha, max_iter=10000)\n",
    "    elif model_type == 'Ridge':\n",
    "        return Ridge(alpha=alpha)\n",
    "    elif model_type == 'Elastic':\n",
    "        return ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "    else:\n",
    "        return LinearRegression()\n",
    "\n",
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features from input data.\"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    return poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(X, y, test_degree, n_folds=5, model_type='None', alpha=1.0, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for polynomial regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        Input features\n",
    "    y : ndarray\n",
    "        Target values\n",
    "    test_degree : int\n",
    "        Degree of polynomial to test\n",
    "    n_folds : int\n",
    "        Number of folds for cross-validation\n",
    "    model_type : str\n",
    "        Type of regularization to use\n",
    "    alpha : float\n",
    "        Regularization strength\n",
    "    l1_ratio : float\n",
    "        Mixing parameter for ElasticNet\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cv_results : dict\n",
    "        Dictionary with cross-validation results\n",
    "    \"\"\"\n",
    "    # Create polynomial features\n",
    "    X_poly = create_polynomial_features(X, test_degree)\n",
    "    \n",
    "    # Initialize k-fold cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    fold_coefs = []\n",
    "    fold_intercepts = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(X_poly)):\n",
    "        # Split data into train and validation sets\n",
    "        X_train, X_val = X_poly[train_idx], X_poly[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = get_model(model_type, alpha, l1_ratio)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate train and validation losses\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "        val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "        \n",
    "        # Store results\n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_val_losses.append(val_loss)\n",
    "        \n",
    "        # Store model parameters\n",
    "        if hasattr(model, 'coef_'):\n",
    "            fold_coefs.append(model.coef_)\n",
    "        else:\n",
    "            fold_coefs.append(None)\n",
    "            \n",
    "        if hasattr(model, 'intercept_'):\n",
    "            fold_intercepts.append(model.intercept_)\n",
    "        else:\n",
    "            fold_intercepts.append(None)\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = np.mean(fold_train_losses)\n",
    "    avg_val_loss = np.mean(fold_val_losses)\n",
    "    \n",
    "    # Create dictionary with results\n",
    "    cv_results = {\n",
    "        'fold_train_losses': fold_train_losses,\n",
    "        'fold_val_losses': fold_val_losses,\n",
    "        'fold_coefs': fold_coefs,\n",
    "        'fold_intercepts': fold_intercepts,\n",
    "        'avg_train_loss': avg_train_loss,\n",
    "        'avg_val_loss': avg_val_loss\n",
    "    }\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "def train_test_model(X, y, test_degree, test_size=0.2, model_type='None', alpha=1.0, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Train model on train set and evaluate on test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        Input features\n",
    "    y : ndarray\n",
    "        Target values\n",
    "    test_degree : int\n",
    "        Degree of polynomial to test\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    model_type : str\n",
    "        Type of regularization to use\n",
    "    alpha : float\n",
    "        Regularization strength\n",
    "    l1_ratio : float\n",
    "        Mixing parameter for ElasticNet\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    test_results : dict\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Create polynomial features\n",
    "    X_poly = create_polynomial_features(X, test_degree)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_poly, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = get_model(model_type, alpha, l1_ratio)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate train and test losses\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Create dictionary with results\n",
    "    test_results = {\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_polynomial_regression():\n",
    "    @interact(\n",
    "        true_degree=widgets.IntSlider(min=1, max=9, step=1, value=3, description='True Degree:'),\n",
    "        noise_level=widgets.FloatSlider(min=0.1, max=20.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "        n_samples=widgets.IntSlider(min=20, max=200, step=10, value=100, description='Sample Size:'),\n",
    "        test_degree=widgets.IntSlider(min=1, max=15, step=1, value=3, description='Test Degree:'),\n",
    "        regularization=widgets.RadioButtons(\n",
    "            options=['None', 'Lasso', 'Ridge', 'Elastic'],\n",
    "            value='None',\n",
    "            description='Regularization:'\n",
    "        ),\n",
    "        alpha=widgets.FloatLogSlider(\n",
    "            min=-5, max=1, step=0.1, value=0.1, base=10, description='Alpha (Reg. Strength):'\n",
    "        ),\n",
    "        l1_ratio=widgets.FloatSlider(\n",
    "            min=0.0, max=1.0, step=0.05, value=0.5, description='L1 Ratio (Elastic):'\n",
    "        ),\n",
    "        n_folds=widgets.IntSlider(min=3, max=10, step=1, value=5, description='CV Folds:')\n",
    "    )\n",
    "    def cross_validate_and_visualize(true_degree, noise_level, n_samples, test_degree,\n",
    "                                    regularization, alpha, l1_ratio, n_folds):\n",
    "        # Generate synthetic data\n",
    "        X, y, true_coef = generate_data(n_samples, true_degree, noise_level)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_results = perform_cross_validation(\n",
    "            X, y, test_degree, n_folds, regularization, alpha, l1_ratio\n",
    "        )\n",
    "        \n",
    "        # Train final model on all data and evaluate on test set\n",
    "        test_results = train_test_model(\n",
    "            X, y, test_degree, 0.2, regularization, alpha, l1_ratio\n",
    "        )\n",
    "        \n",
    "        # Create figure with 3 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "        \n",
    "        # Plot 1: Data and model fit\n",
    "        ax1 = axes[0, 0]\n",
    "        \n",
    "        # Plot original data\n",
    "        X_flat = X.flatten()\n",
    "        ax1.scatter(X_flat, y, alpha=0.6, label='Data points')\n",
    "        \n",
    "        # Plot true function\n",
    "        X_line = np.linspace(min(X_flat), max(X_flat), 100).reshape(-1, 1)\n",
    "        y_true = np.zeros(100)\n",
    "        for i in range(true_degree + 1):\n",
    "            y_true += true_coef[i] * X_line.flatten()**i\n",
    "        ax1.plot(X_line, y_true, 'r-', linewidth=2, label='True function')\n",
    "        \n",
    "        # Plot model fit\n",
    "        X_poly_line = create_polynomial_features(X_line, test_degree)\n",
    "        y_pred = test_results['model'].predict(X_poly_line)\n",
    "        ax1.plot(X_line, y_pred, 'g-', linewidth=2, label=f'Model fit (degree={test_degree})')\n",
    "        \n",
    "        ax1.set_title(f'Data and Model Fit\\nTrue degree: {true_degree}, Test degree: {test_degree}')\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: Validation loss for each fold\n",
    "        ax2 = axes[0, 1]\n",
    "        \n",
    "        folds = list(range(1, n_folds + 1))\n",
    "        ax2.bar(\n",
    "            [f - 0.2 for f in folds], \n",
    "            cv_results['fold_train_losses'], \n",
    "            width=0.4, \n",
    "            color='blue', \n",
    "            alpha=0.6, \n",
    "            label='Train Loss'\n",
    "        )\n",
    "        ax2.bar(\n",
    "            [f + 0.2 for f in folds], \n",
    "            cv_results['fold_val_losses'], \n",
    "            width=0.4, \n",
    "            color='red', \n",
    "            alpha=0.6, \n",
    "            label='Validation Loss'\n",
    "        )\n",
    "        \n",
    "        ax2.axhline(\n",
    "            cv_results['avg_train_loss'], \n",
    "            color='blue', \n",
    "            linestyle='--', \n",
    "            alpha=0.8,\n",
    "            label=f'Avg Train Loss: {cv_results[\"avg_train_loss\"]:.4f}'\n",
    "        )\n",
    "        ax2.axhline(\n",
    "            cv_results['avg_val_loss'], \n",
    "            color='red', \n",
    "            linestyle='--', \n",
    "            alpha=0.8,\n",
    "            label=f'Avg Val Loss: {cv_results[\"avg_val_loss\"]:.4f}'\n",
    "        )\n",
    "        ax2.axhline(\n",
    "            test_results['test_loss'], \n",
    "            color='green', \n",
    "            linestyle='--', \n",
    "            alpha=0.8,\n",
    "            label=f'Test Loss: {test_results[\"test_loss\"]:.4f}'\n",
    "        )\n",
    "        \n",
    "        ax2.set_title(f'Train and Validation Loss for Each Fold\\n({n_folds}-fold Cross-Validation)')\n",
    "        ax2.set_xlabel('Fold')\n",
    "        ax2.set_ylabel('Mean Squared Error')\n",
    "        ax2.set_xticks(folds)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Plot 3: Model parameters for each fold\n",
    "        ax3 = axes[1, 0]\n",
    "        \n",
    "        # Get coefficients for each fold\n",
    "        coefs_array = np.array(cv_results['fold_coefs'])\n",
    "        \n",
    "        # Plot coefficients\n",
    "        for i in range(coefs_array.shape[1]):\n",
    "            label = f'Coef {i}' if i > 0 else 'Intercept'\n",
    "            ax3.plot(folds, coefs_array[:, i], 'o-', label=label)\n",
    "        \n",
    "        ax3.set_title('Model Parameters for Each Fold')\n",
    "        ax3.set_xlabel('Fold')\n",
    "        ax3.set_ylabel('Coefficient Value')\n",
    "        ax3.set_xticks(folds)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Plot 4: Train, validation, test loss comparison across model complexities\n",
    "        ax4 = axes[1, 1]\n",
    "        \n",
    "        # Evaluate models with different degrees\n",
    "        degrees = list(range(1, test_degree + 5))\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for degree in degrees:\n",
    "            # Perform cross-validation\n",
    "            cv_res = perform_cross_validation(\n",
    "                X, y, degree, n_folds, regularization, alpha, l1_ratio\n",
    "            )\n",
    "            \n",
    "            # Train final model on all data and evaluate on test set\n",
    "            test_res = train_test_model(\n",
    "                X, y, degree, 0.2, regularization, alpha, l1_ratio\n",
    "            )\n",
    "            \n",
    "            train_losses.append(cv_res['avg_train_loss'])\n",
    "            val_losses.append(cv_res['avg_val_loss'])\n",
    "            test_losses.append(test_res['test_loss'])\n",
    "        \n",
    "        ax4.plot(degrees, train_losses, 'o-', color='blue', label='Train Loss')\n",
    "        ax4.plot(degrees, val_losses, 'o-', color='red', label='Validation Loss')\n",
    "        ax4.plot(degrees, test_losses, 'o-', color='green', label='Test Loss')\n",
    "        \n",
    "        ax4.axvline(\n",
    "            true_degree, \n",
    "            color='black', \n",
    "            linestyle='--', \n",
    "            alpha=0.5,\n",
    "            label=f'True Degree: {true_degree}'\n",
    "        )\n",
    "        \n",
    "        ax4.set_title('Model Performance vs. Complexity')\n",
    "        ax4.set_xlabel('Polynomial Degree')\n",
    "        ax4.set_ylabel('Mean Squared Error')\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"\\n=== Cross-Validation Results ===\\n\")\n",
    "        \n",
    "        # Create a DataFrame for fold-by-fold results\n",
    "        fold_results = pd.DataFrame({\n",
    "            'Fold': list(range(1, n_folds + 1)),\n",
    "            'Train Loss': cv_results['fold_train_losses'],\n",
    "            'Validation Loss': cv_results['fold_val_losses']\n",
    "        })\n",
    "        \n",
    "        print(fold_results)\n",
    "        \n",
    "        print(f\"\\nAverage Train Loss: {cv_results['avg_train_loss']:.4f}\")\n",
    "        print(f\"Average Validation Loss: {cv_results['avg_val_loss']:.4f}\")\n",
    "        print(f\"Test Loss: {test_results['test_loss']:.4f}\")\n",
    "        \n",
    "        print(\"\\n=== Model Parameters for Each Fold ===\\n\")\n",
    "        \n",
    "        # Create a DataFrame for model parameters\n",
    "        param_cols = [f'Coef {i}' if i > 0 else 'Intercept' for i in range(coefs_array.shape[1])]\n",
    "        param_data = pd.DataFrame(coefs_array, columns=param_cols)\n",
    "        param_data.insert(0, 'Fold', list(range(1, n_folds + 1)))\n",
    "        \n",
    "        print(param_data)\n",
    "        \n",
    "        # Print true coefficients\n",
    "        print(\"\\n=== True Coefficients ===\\n\")\n",
    "        true_coef_names = [f'Coef {i}' if i > 0 else 'Intercept' for i in range(len(true_coef))]\n",
    "        true_coef_df = pd.DataFrame([true_coef], columns=true_coef_names)\n",
    "        print(true_coef_df)\n",
    "        \n",
    "        # Print final model coefficients\n",
    "        print(\"\\n=== Final Model Coefficients ===\\n\")\n",
    "        final_model = test_results['model']\n",
    "        final_coef = np.concatenate(([final_model.intercept_], final_model.coef_[1:]))\n",
    "        final_coef_names = [f'Coef {i}' if i > 0 else 'Intercept' for i in range(len(final_coef))]\n",
    "        final_coef_df = pd.DataFrame([final_coef], columns=final_coef_names)\n",
    "        print(final_coef_df)\n",
    "\n",
    "        # Explain findings\n",
    "        print(\"\\n=== Interpretation ===\\n\")\n",
    "        \n",
    "        # Compare true degree vs test degree\n",
    "        if true_degree == test_degree:\n",
    "            print(f\"The test polynomial degree ({test_degree}) matches the true degree ({true_degree}).\")\n",
    "        elif test_degree < true_degree:\n",
    "            print(f\"The test polynomial degree ({test_degree}) is lower than the true degree ({true_degree}).\")\n",
    "            print(\"This suggests the model is underfit (high bias), which can be seen in the error plots.\")\n",
    "        else:\n",
    "            print(f\"The test polynomial degree ({test_degree}) is higher than the true degree ({true_degree}).\")\n",
    "            print(\"This suggests potential overfitting if regularization is not applied properly.\")\n",
    "        \n",
    "        # Analyze validation and test loss\n",
    "        val_test_diff = abs(cv_results['avg_val_loss'] - test_results['test_loss'])\n",
    "        if val_test_diff < 0.1 * cv_results['avg_val_loss']:\n",
    "            print(\"\\nThe cross-validation estimate closely matches the test loss.\")\n",
    "            print(\"This suggests the model generalizes well to unseen data.\")\n",
    "        else:\n",
    "            print(\"\\nThere's a notable difference between cross-validation and test loss.\")\n",
    "            print(\"This may indicate high variance in the data or instability in the model.\")\n",
    "        \n",
    "        # Analyze cross-validation stability\n",
    "        cv_std = np.std(cv_results['fold_val_losses'])\n",
    "        cv_mean = np.mean(cv_results['fold_val_losses'])\n",
    "        cv_variation = cv_std / cv_mean\n",
    "        \n",
    "        if cv_variation < 0.1:\n",
    "            print(\"\\nThe cross-validation results are very stable across folds.\")\n",
    "        elif cv_variation < 0.3:\n",
    "            print(\"\\nThe cross-validation results show moderate variation across folds.\")\n",
    "        else:\n",
    "            print(\"\\nThe cross-validation results show high variation across folds.\")\n",
    "            print(\"This may suggest the model is sensitive to the specific data split.\")\n",
    "        \n",
    "        # Regularization effects\n",
    "        if regularization != 'None':\n",
    "            print(f\"\\nRegularization ({regularization}) was applied with alpha={alpha:.6f}.\")\n",
    "            if regularization == 'Elastic':\n",
    "                print(f\"L1 ratio was set to {l1_ratio:.2f} (balance between Lasso and Ridge).\")\n",
    "            \n",
    "            # Check if regularization is helping\n",
    "            degree_diff = test_degree - true_degree\n",
    "            if degree_diff > 2 and test_losses[test_degree-1] < 1.5 * test_losses[true_degree-1]:\n",
    "                print(\"Regularization appears to be effectively preventing overfitting.\")\n",
    "            elif degree_diff > 2:\n",
    "                print(\"Despite regularization, there may still be overfitting.\")\n",
    "                if alpha < 0.1:\n",
    "                    print(\"Consider increasing the regularization strength (alpha).\")\n",
    "\n",
    "# Run the interactive application\n",
    "interactive_polynomial_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
